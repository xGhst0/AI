#!/usr/bin/env bash
set -euo pipefail

CONFIG_DIR="$HOME/.ai_cli_offline/scheduled_learning"
CONFIG_FILE="$CONFIG_DIR/config.yaml"
SCRIPT_PATH="$CONFIG_DIR/scheduled_learning.py"
SERVICE_PATH="/etc/systemd/system/ai-scheduled-learning.service"
TIMER_PATH="/etc/systemd/system/ai-scheduled-learning.timer"

echo "[INFO] Setting up Scheduled Learning feature..."

mkdir -p "$CONFIG_DIR"

# Default config: topic and interval (daily)
cat > "$CONFIG_FILE" <<EOF
# Scheduled Learning Config
# Define topics and intervals in cron syntax

topics:
  - name: cybersecurity
    query: cybersecurity
    interval: daily
EOF

echo "[INFO] Created default config at $CONFIG_FILE"

# Install python deps for scraping, summarization, and yaml parsing
python3 -m pip install --user duckduckgo_search beautifulsoup4 pyyaml nltk

echo "[INFO] Installing scheduled learning script..."

cat > "$SCRIPT_PATH" <<'PYTHON_SCRIPT'
#!/usr/bin/env python3
import os
import yaml
import time
from duckduckgo_search import DDGS
import requests
from bs4 import BeautifulSoup
import nltk
from nltk.tokenize import sent_tokenize

# Download punkt tokenizer for nltk (once)
nltk.download('punkt')

CONFIG_FILE = os.path.expanduser("~/.ai_cli_offline/scheduled_learning/config.yaml")
KNOWLEDGE_FILE = os.path.expanduser("~/.ai_cli_offline/scheduled_learning/knowledge_base.txt")

def summarize_text(text, max_sentences=5):
    sentences = sent_tokenize(text)
    return ' '.join(sentences[:max_sentences])

def crawl_and_learn(query):
    print(f"[Scheduled Learning] Searching for: {query}")
    summaries = []
    try:
        with DDGS() as ddgs:
            results = list(ddgs.text(query, max_results=3))
            for r in results:
                url = r.get('href')
                if not url:
                    continue
                print(f"[Scheduled Learning] Fetching: {url}")
                try:
                    resp = requests.get(url, timeout=10)
                    soup = BeautifulSoup(resp.text, 'html.parser')
                    paragraphs = soup.find_all('p')
                    text = ' '.join(p.get_text() for p in paragraphs)
                    summary = summarize_text(text)
                    summaries.append(f"URL: {url}\nSummary: {summary}\n")
                except Exception as e:
                    print(f"[Scheduled Learning] Failed to fetch {url}: {e}")
    except Exception as e:
        print(f"[Scheduled Learning] Search failed: {e}")
    return summaries

def load_config():
    with open(CONFIG_FILE, 'r') as f:
        return yaml.safe_load(f)

def append_to_knowledge(summaries):
    os.makedirs(os.path.dirname(KNOWLEDGE_FILE), exist_ok=True)
    with open(KNOWLEDGE_FILE, 'a') as f:
        for summary in summaries:
            f.write(summary + "\n---\n")

def main():
    config = load_config()
    for topic in config.get('topics', []):
        query = topic.get('query')
        if query:
            summaries = crawl_and_learn(query)
            if summaries:
                append_to_knowledge(summaries)
                print(f"[Scheduled Learning] Added {len(summaries)} summaries for topic '{query}'")

if __name__ == "__main__":
    main()
PYTHON_SCRIPT

chmod +x "$SCRIPT_PATH"

echo "[INFO] Creating systemd service and timer..."

sudo tee "$SERVICE_PATH" > /dev/null <<EOF
[Unit]
Description=AI CLI Scheduled Learning Service
After=network.target

[Service]
Type=simple
User=$USER
ExecStart=$SCRIPT_PATH
Restart=on-failure

[Install]
WantedBy=multi-user.target
EOF

sudo tee "$TIMER_PATH" > /dev/null <<EOF
[Unit]
Description=Runs AI CLI Scheduled Learning Daily

[Timer]
OnCalendar=daily
Persistent=true

[Install]
WantedBy=timers.target
EOF

echo "[INFO] Enabling and starting timer..."
sudo systemctl daemon-reload
sudo systemctl enable --now ai-scheduled-learning.timer

echo "[SUCCESS] Scheduled Learning feature installed and enabled."
echo "You can edit topics and intervals in $CONFIG_FILE"
echo "Knowledge base stored at ~/.ai_cli_offline/scheduled_learning/knowledge_base.txt"
